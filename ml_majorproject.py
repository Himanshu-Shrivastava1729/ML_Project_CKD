# -*- coding: utf-8 -*-
"""ML_MajorProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dIcHti2ze5ZZIL4Ck2lWWXCw3-SyOPyL
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder
from scipy.stats import zscore
pd.set_option('display.max_columns', None)

file_path = "/content/kidney_disease.csv"
df = pd.read_csv(file_path)

df.shape

df.describe()

df['classification'].unique()

df["classification"] = df["classification"].replace("ckd\t","ckd")

df['classification'].value_counts()

df.drop('id', axis=1, inplace=True)
df.head()

# Below we'll change classifictaion column to binary values 0 or 1.
classification_new = []
for val in df["classification"]:
  if val == "ckd":
    classification_new.append(1)
  else:
    classification_new.append(0)

df["classification"] = classification_new
df['classification'].value_counts()

df['classification'].value_counts().plot(kind='bar')

# Now we're showing missing values in our dataset.
print("\nMissing values count:")
print(df.isnull().sum())

# Here we're plotting distro so that we can compare later
# with these distro whether imputation and normalization has
# changed distro or not.
num_cols = df.select_dtypes(include=['float64', 'int64']).columns
cat_cols = df.select_dtypes(include=['object']).columns
print("Numerical columns:")
print(num_cols)
print("\nCategorical columns:")
print(cat_cols)
for col in num_cols:
    plt.figure(figsize=(6, 4))
    sns.histplot(df[col], kde=True, bins=20)
    plt.title(f"Distribution of {col}")
    plt.show()

# Here we're cleaning dataset
df['pcv'] = df['pcv'].replace('', '0')
df['rc'] = df['rc'].replace('', '0')
df['wc'] = df['wc'].replace('', '0')
df['pcv'] = df['pcv'].replace('\t?', '')
df['rc'] = df['rc'].replace('\t?', '')
df['wc'] = df['wc'].replace('\t?', '')
df['pcv'] = df['pcv'].replace('\t43', '43')
df['wc'] = df['wc'].replace('\t6200', '6200')
df['wc'] = df['wc'].replace('\t8400', '8400')

# Below we're doing imputation for numerical cols,if distro is almost normal use mean
# Otherwise use median.
for col in num_cols:
    if df[col].isnull().sum() > 0:
        skewness = df[col].skew()
        if abs(skewness) < 0.5:
            df[col] = df[col].fillna(df[col].mean())
        else:
            df[col] = df[col].fillna(df[col].median())

print(df.isnull().sum())

categorical_columns = ['rbc', 'pc','pcc','ba', 'htn','dm','cad','appet','pe','ane']
df[categorical_columns].isnull().mean()*100

# For categorical cols we're replacing missing values with mode.
for i in categorical_columns:
    print(i, ": ", df[i].mode())

df['rbc'] = df['rbc'].fillna('normal')
df['pc'] = df['pc'].fillna('normal')
df['pcc'] = df['pcc'].fillna('notpresent')
df['ba'] = df['ba'].fillna('notpresent')
df['htn'] = df['htn'].fillna('no')
df['dm'] = df['dm'].fillna('no')
df['cad'] = df['cad'].fillna('no')
df['appet'] = df['appet'].fillna('good')
df['pe'] = df['pe'].fillna('no')
df['ane'] = df['ane'].fillna('no')

df['pcv'] = df['pcv'].astype(str).str.strip()
df['pcv'] = df['pcv'].replace(['', '\t?', '\t43'], ['0', '0', '43']).replace(r'[^\d]', '0', regex=True).astype(int)

df['wc'] = df['wc'].astype(str).str.strip()
df['wc'] = df['wc'].replace(['', '\t?', '\t6200', '\t8400'], ['0', '0', '6200', '8400']).replace(r'[^\d]', '0', regex=True).astype(int)

df['rc'] = df['rc'].astype(str).str.strip()
df['rc'] = df['rc'].replace(['', '\t?'], ['0', '0']).replace(r'[^\d.]', '0', regex=True).astype(float)

df['dm'] = df['dm'].str.strip()
df['cad'] = df['cad'].str.strip()

# Replacing NaN values
df['pcv'] = df['pcv'].fillna(0).astype(int)
df['wc'] = df['wc'].fillna(0).astype(int)
df['rc'] = df['rc'].fillna(0.0).astype(float)
df.info()

cat_mapping = {
        "rbc": {
        "abnormal":1,
        "normal": 0,
    },
        "pc":{
        "abnormal":1,
        "normal": 0,
    },
        "pcc":{
        "present":1,
        "notpresent":0,
    },
        "ba":{
        "notpresent":0,
        "present": 1,
    },
        "htn":{
        "yes":1,
        "no": 0,
    },
        "dm":{
        "yes":1,
        "no":0,
    },
        "cad":{
        "yes":1,
        "no": 0,
    },
        "appet":{
        "good":1,
        "poor": 0,
    },
        "pe":{
        "yes":1,
        "no":0,
    },
        "ane":{
        "yes":1,
        "no":0,
    }
}

# Here we're converting categorical cols into binary values
df[categorical_columns] = df[categorical_columns].replace(cat_mapping)

df.sample(10)

plt.figure(figsize = (20,20))
sns.heatmap(df.corr(), annot = True, fmt=".2f",linewidths=0.5)

df.corr()

df.sample(5)

numerical_cols = ['age', 'bp', 'sg', 'al', 'su', 'bgr', 'bu', 'sc', 'sod', 'pot', 'hemo','pcv','wc','rc']

# Here we're normalizing cols.
def min_max_normalization(column):
    min_val = column.min()
    max_val = column.max()
    return (column - min_val) / (max_val - min_val)

for col in numerical_cols:
    df[col] = min_max_normalization(df[col])

df.head()

# Again checking if normalization and imputation has not changed distro drastically.
num_cols = df.select_dtypes(include=['float64', 'int64']).columns
cat_cols = df.select_dtypes(include=['object']).columns
print("Numerical columns:")
print(num_cols)
print("\nCategorical columns:")
print(cat_cols)
for col in num_cols:
    plt.figure(figsize=(6, 4))
    sns.histplot(df[col], kde=True, bins=20)
    plt.title(f"Distribution of {col}")
    plt.show()

# Below we're implementing ANN.
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,f1_score

# We're defining features (X) and target (y)
X = df.drop('classification', axis=1).values
y = df['classification'].values

# We will split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# We're initializing weights and biases for a 2-layer ANN
np.random.seed(42)
input_size = X_train.shape[1]
hidden_size = 64  # Number of neurons in the hidden layer
output_size = 1  # Binary classification

# Weights and biases
W1 = np.random.randn(input_size, hidden_size) * 0.01
b1 = np.zeros((1, hidden_size))
W2 = np.random.randn(hidden_size, output_size) * 0.01
b2 = np.zeros((1, output_size))

# We're taking activation functions Relu and sigmoid.
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return sigmoid(x) * (1 - sigmoid(x))

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return np.where(x > 0, 1, 0)

# Function for forward propagation
def forward_propagation(X):
    global W1, b1, W2, b2
    Z1 = np.dot(X, W1) + b1
    A1 = relu(Z1)
    Z2 = np.dot(A1, W2) + b2
    A2 = sigmoid(Z2)
    # A2 =relu(Z2) accuracy going down beacause output is of binary type so sigmoid is used.
    return Z1, A1, Z2, A2

# Function for backpropagation
def backward_propagation(X, y, Z1, A1, Z2, A2, learning_rate=0.01):
    global W1, b1, W2, b2
    m = X.shape[0]

    # Now we'll compute gradients
    dZ2 = A2 - y.reshape(-1, 1)
    dW2 = np.dot(A1.T, dZ2) / m
    db2 = np.sum(dZ2, axis=0, keepdims=True) / m

    dA1 = np.dot(dZ2, W2.T)
    dZ1 = dA1 * relu_derivative(Z1)
    dW1 = np.dot(X.T, dZ1) / m
    db1 = np.sum(dZ1, axis=0, keepdims=True) / m

    # Update weights and biases
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2

# Now train the ANN
epochs = 50
learning_rate = 0.01

for epoch in range(epochs):
    # Forward propagation
    Z1, A1, Z2, A2 = forward_propagation(X_train)

    # Compute loss (binary cross-entropy)
    loss = -np.mean(y_train * np.log(A2.flatten() + 1e-8) + (1 - y_train) * np.log(1 - A2.flatten() + 1e-8))

    # Backward propagation
    backward_propagation(X_train, y_train, Z1, A1, Z2, A2, learning_rate)

    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss:.4f}")

# Evaluate the model,accuracy and F1 score.
_, _, _, A2_test = forward_propagation(X_test)
y_pred = (A2_test > 0.5).astype(int).flatten()
accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")


f1 = f1_score(y_test, y_pred)


print(f"F1 Score: {f1:.4f}")

import numpy as np

# Below we're defining the hinge loss function for SVM
def hinge_loss(X, y, weights, bias):
    margin = y * (np.dot(X, weights) + bias)
    loss = np.maximum(0, 1 - margin)  # Hinge loss for incorrectly classified points
    return np.mean(loss) + 0.5 * np.dot(weights, weights)  # Add L2 regularization

# Below we've gradient descent function for optimization
def train_svm(X, y, learning_rate=0.001, epochs=1000, reg_strength=0.01):
    num_samples, num_features = X.shape
    weights = np.zeros(num_features)
    bias = 0

    for epoch in range(epochs):
        # Gradient calculations
        margins = y * (np.dot(X, weights) + bias)
        misclassified = margins < 1
        #misclassified is acting as indicator R.V.

        dw = reg_strength * weights - np.dot(X.T, y * misclassified) / num_samples
        db = -np.sum(y * misclassified) / num_samples

        # Now update weights and bias
        weights -= learning_rate * dw
        bias -= learning_rate * db

        # Print loss every 100 epochs
        if epoch % 100 == 0:
            loss = hinge_loss(X, y, weights, bias)
            print(f"Epoch {epoch}, Loss: {loss:.4f}")

    return weights, bias

# Prediction function
def predict(X, weights, bias):
    linear_output = np.dot(X, weights) + bias
    return np.where(linear_output >= 0, 1, -1)

# Example dataset in pandas DataFrame
import pandas as pd
from sklearn.model_selection import train_test_split

# Assuming df is a pandas DataFrame with binary classification
# Map classification to -1 and 1
df['classification'] = df['classification'].map({0: -1, 1: 1})

# Define features and target
X = df.drop('classification', axis=1).values
y = df['classification'].values

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train SVM
weights, bias = train_svm(X_train, y_train, learning_rate=0.01, epochs=1000, reg_strength=0.01)

# Evaluate SVM using accuracy and F1 score:
y_pred = predict(X_test, weights, bias)
accuracy = np.mean(y_pred == y_test)
print(f"Test Accuracy: {accuracy:.4f}")
f1 = f1_score(y_test, y_pred)
print(f"F1 Score: {f1:.4f}")